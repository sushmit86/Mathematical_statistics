library(pls)
library(ISLR)
library(leaps)
attach(Hitters)
library(glmnet)
library(pls)
betas = seq(10,10.0.1)
betas = seq(10,10, 0.1)
betas = seq(10,10, 0.1)
baetas
betas
betas = seq(10,10, 0.1)
betas
betas[1]
betas[2]
y = 2
lambda = 2
betas = seq(-10,10, 0.1)
betas
plot(betas, func, pch = 20, xlab = "beta", ylab = "Ridge optimization")
func = ( y - betas)^2 + lambda * betas^2
plot(betas, func, pch = 20, xlab = "beta", ylab = "Ridge optimization")
est.beta = y/(1 + lambda)
est.func = (y - est.beta)^2 + lambda * est.beta^2
points(est.beta, est.func, col = "red", pch = 4, lwd = 5, cex = est.beta)
betas = seq(-3,3,0.01)
func = ( y - betas)^2 + lambda * abs(betas)
plot(betas, func, pch = 20, xlab = "beta", ylab = "Lasso optimization")
est.beta = y - lambda/2
est.beta = y - lambda/2
est.func = (y - est.beta)^2 + lambda * abs(est.beta)
points(est.beta, est.func, col = "red", pch = 4, lwd = 5, cex = est.beta)
points(est.beta, est.func, col = "red", pch = 4, lwd = 5, cex = est.beta)
points(est.beta, est.func, col = "red", pch = 4, lwd = 5, cex = est.beta)
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
beta0 = 3
beta1 = 2
beta2 = -3
beta3 = 0.3
Y = beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps
library(leaps)
data.full = data.frame(y = Y, x = X)
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
beta0 = 3
beta1 = 2
beta2 = -3
beta3 = 0.3
Y = beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps
data.full = data.frame(y = Y, x = X)
mod.full = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10)
mod.summary = summary(mod.full)
# Find the model size for best cp, BIC and adjr2
which.min(mod.summary$cp)
which.min(mod.summary$bic)
which.max(mod.summary$adjr2)
source('~/Stat_learning/Chater6_Exercise.R', echo=TRUE)
which.min(mod.summary$aic)
plot(mod.summary$cp, xlab = "Subset Size", ylab = "Cp", pch = 20, type = "l")
points(which.min(mod.summary$cp), mod.summary$cp[which.min(mod.summary$cp)], pch = 4, col = "red", lwd = 7)
plot(mod.summary$bic, xlab = "Subset Size", ylab = "BIC", pch = 20, type = "l")
points(3, mod.summary$bic[3], pch = 4, col = "red", lwd = 7)
plot(mod.summary$adjr2, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20,
type = "l")
points(3, mod.summary$adjr2[3], pch = 4, col = "red", lwd = 7)
coefficients(mod.full, id = 3)
mod.fwd = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10,  method = "forward")
mod.bwd = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10,  method = "backward")
fwd.summary = summary(mod.fwd)
bwd.summary = summary(mod.bwd)
which.min(fwd.summary$cp)
source('~/Stat_learning/Chater6_Exercise.R', echo=TRUE)
which.min(bwd.summary$bic)
which.max(fwd.summary$adjr2)
source('~/Stat_learning/Chater6_Exercise.R', echo=TRUE)
# Plot the statistics
par(mfrow = c(3, 2))
plot(fwd.summary$cp, xlab = "Subset Size", ylab = "Forward Cp", pch = 20, type = "l")
points(3, fwd.summary$cp[3], pch = 4, col = "red", lwd = 7)
plot(bwd.summary$cp, xlab = "Subset Size", ylab = "Backward Cp", pch = 20, type = "l")
points(3, bwd.summary$cp[3], pch = 4, col = "red", lwd = 7)
plot(fwd.summary$bic, xlab = "Subset Size", ylab = "Forward BIC", pch = 20,
type = "l")
points(3, fwd.summary$bic[3], pch = 4, col = "red", lwd = 7)
plot(bwd.summary$bic, xlab = "Subset Size", ylab = "Backward BIC", pch = 20,
type = "l")
points(3, bwd.summary$bic[3], pch = 4, col = "red", lwd = 7)
plot(fwd.summary$adjr2, xlab = "Subset Size", ylab = "Forward Adjusted R2",
pch = 20, type = "l")
points(3, fwd.summary$adjr2[3], pch = 4, col = "red", lwd = 7)
plot(bwd.summary$adjr2, xlab = "Subset Size", ylab = "Backward Adjusted R2",
pch = 20, type = "l")
points(4, bwd.summary$adjr2[4], pch = 4, col = "red", lwd = 7)
coefficients(mod.fwd, id = 3)
coefficients(mod.bwd, id = 3)
coefficients(mod.fwd, id = 4)
library(glmnet)
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data.full)[, -1]
xmat
y ~ poly(x, 10, raw = T)
summary(y ~ poly(x, 10, raw = T))
x
library(ISLR)
library(leaps)
attach(Hitters)
library(glmnet)
library(pls)
library(leaps)
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
beta1 = 2
beta2 = -3
beta3 = 0.3
Y = beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps
data.full = data.frame(y = Y, x = X)
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
beta0 = 3
beta1 = 2
beta2 = -3
beta3 = 0.3
Y = beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps
data.full = data.frame(y = Y, x = X)
mod.full = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10)
mod.summary = summary(mod.full)
which.min(mod.summary$cp)
which.min(mod.summary$aic)
which.max(mod.summary$adjr2)
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data.full)[, -1]
x
y
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data.full)[, -1]
xmat
model.matrix(y ~ poly(x, 10, raw = T), data = data.full)
mod.lasso = cv.glmnet(xmat, Y, alpha = 1)
best.lambda = mod.lasso$lambda.min
best.lambda
best.lambda = mod.lasso$lambda.min
best.lambda
plot(mod.lasso)
par(mfrow = c(1, 1))
plot(mod.lasso)
best.model = glmnet(xmat, Y, alpha = 1)
predict(best.model, s = best.lambda, type = "coefficients")
Y = beta0 + beta7 * X^7 + eps
beta7 = 7
Y = beta0 + beta7 * X^7 + eps
Y = beta0 + beta7 * X^7 + eps
data.full = data.frame(y = Y, x = X)
mod.full = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10)
mod.summary = summary(mod.full)
which.min(mod.summary$cp)
which.min(mod.summary$bic)
which.max(mod.summary$adjr2)
coefficients(mod.full, id = 1)
coefficients(mod.full, id = 2)
coefficients(mod.full, id = 7)
coefficients(mod.full, id = 4)
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data.full)[, -1]
mod.lasso = cv.glmnet(xmat, Y, alpha = 1)
best.lambda = mod.lasso$lambda.min
best.lambda
best.model = glmnet(xmat, Y, alpha = 1)
predict(best.model, s = best.lambda, type = "coefficients")
library(ISLR)
set.seed(11)
sum(is.na(College))
head(College)
train.size = dim(College)[1]/2
train = sample(1:dim(College)[1], train.size)
train
test = -train
test
College.train = College[train, ]
College.test = College[test, ]
lm.fit = lm(Apps~., data=College.train)
lm.pred = predict(lm.fit, College.test)
summary(lm.fit)
mean((College.test[, "Apps"] - lm.pred)^2)
library(glmnet)
train.mat = model.matrix(Apps~., data=College.train)
test.mat = model.matrix(Apps~., data=College.test)
grid = 10 ^ seq(4, -2, length=100)
mod.ridge = cv.glmnet(train.mat, College.train[, "Apps"], alpha=0, lambda=grid, thresh=1e-12)
lambda.best = mod.ridge$lambda.min
lambda.best
ridge.pred = predict(mod.ridge, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - ridge.pred)^2)
mod.lasso = cv.glmnet(train.mat, College.train[, "Apps"], alpha=1, lambda=grid, thresh=1e-12)
lambda.best = mod.lasso$lambda.min
lambda.best
lasso.pred = predict(mod.lasso, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - lasso.pred)^2)
mod.lasso = glmnet(model.matrix(Apps~., data=College), College[, "Apps"], alpha=1)
predict(mod.lasso, s=lambda.best, type="coefficients")
library(pls)
pcr.fit = pcr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
pcr.pred = predict(pcr.fit, College.test, ncomp=10)
mean((College.test[, "Apps"] - data.frame(pcr.pred))^2)
pls.fit = plsr(Apps~., data=College.train, scale=T, validation="CV")
pls.fit = plsr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pls.fit, val.type="MSEP")
pls.pred = predict(pls.fit, College.test, ncomp=10)
mean((College.test[, "Apps"] - data.frame(pls.pred))^2)
test.avg = mean(College.test[, "Apps"])
lm.test.r2 = 1 - mean((College.test[, "Apps"] - lm.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
ridge.test.r2 = 1 - mean((College.test[, "Apps"] - ridge.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
lasso.test.r2 = 1 - mean((College.test[, "Apps"] - lasso.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
pcr.test.r2 = 1 - mean((College.test[, "Apps"] - data.frame(pcr.pred))^2) /mean((College.test[, "Apps"] - test.avg)^2)
pls.test.r2 = 1 - mean((College.test[, "Apps"] - data.frame(pls.pred))^2) /mean((College.test[, "Apps"] - test.avg)^2)
barplot(c(lm.test.r2, ridge.test.r2, lasso.test.r2, pcr.test.r2, pls.test.r2), col="red", names.arg=c("OLS", "Ridge", "Lasso", "PCR", "PLS"), main="Test R-squared")
set.seed(1)
p = 20
n = 1000
rnorm(2)
x = matrix(rnorm(n * p), n, p)
B = rnorm(p)
B[3] = 0
B[4] = 0
B[9] = 0
B[19] = 0
B[10] = 0
eps = rnorm(p)
x
B
eps
y = x %*% B + eps
seq(1000)
train = sample(seq(1000), 100, replace = FALSE)
train
y.train = y[train, ]
y.test = y[-train, ]
x.train = x[train, ]
x.test = x[-train, ]
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train), nvmax = p)
val.errors = rep(NA, p)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train), nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train), nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
x_cols
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train), nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
i =5
coefi = coef(regfit.full, id = i)
coefi
x_cols
names(coefi)
x_cols %in% names(coefi)]
x_cols %in% names(coefi)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train), nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
# See lab for easy implementation
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in% x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, ylab = "Training MSE", pch = 19, type = "b")
set.seed(1)
p = 20
n = 1000
x = matrix(rnorm(n * p), n, p)
B = rnorm(p)
B[3] = 0
B[4] = 0
B[9] = 0
B[19] = 0
B[10] = 0
eps = rnorm(p)
y = x %*% B + eps
train = sample(seq(1000), 100, replace = FALSE)
y.train = y[train, ]
y.test = y[-train, ]
x.train = x[train, ]
x.test = x[-train, ]
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train), nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
# See lab for easy implementation
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in% x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, ylab = "Training MSE", pch = 19, type = "b")
val.errors = rep(NA, p)
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, ylab = "Test MSE", pch = 19, type = "b")
which.min(val.errors)
coef(regfit.full, id = 16)
val.errors = rep(NA, p)
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, ylab = "Test MSE", pch = 19, type = "b")
which.min(val.errors)
coef(regfit.full, id = 16)
a = rep(NA, p)
b = rep(NA, p)
val.errors = rep(NA, p)
a = rep(NA, p)
b = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
a[i] = length(coefi) - 1
b[i] = sqrt(sum((B[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(B[!(x_cols %in% names(coefi))])^2)
}
plot(x = a, y = b, xlab = "number of coefficients", ylab = "error between estimated and true coefficients")
which.min(b)
set.seed(1)
predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id = id)
mat[, names(coefi)] %*% coefi
}
k = 10
p = ncol(Boston) - 1
library(MASS)
folds = sample(rep(1:k, length = nrow(Boston)))
cv.errors = matrix(NA, k, p)
for (i in 1:k) {
best.fit = regsubsets(crim ~ ., data = Boston[folds != i, ], nvmax = p)
for (j in 1:p) {
pred = predict(best.fit, Boston[folds == i, ], id = j)
cv.errors[i, j] = mean((Boston$crim[folds == i] - pred)^2)
}
}
for (i in 1:k) {
best.fit = regsubsets(crim ~ ., data = Boston[folds != i, ], nvmax = p)
for (j in 1:p) {
pred = predict(best.fit, Boston[folds == i, ], id = j)
cv.errors[i, j] = mean((Boston$crim[folds == i] - pred)^2)
}
}
for (i in 1:k) {
best.fit = regsubsets(crim ~ ., data = Boston[folds != i, ], nvmax = p)
for (j in 1:p) {
pred = predict(best.fit, Boston[folds == i, ], id = j)
cv.errors[i, j] = mean((Boston$crim[folds == i] - pred)^2)
}
}
predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id = id)
mat[, names(coefi)] %*% coefi
}
k = 10
p = ncol(Boston) - 1
folds = sample(rep(1:k, length = nrow(Boston)))
cv.errors = matrix(NA, k, p)
exit()
for (i in 1:k) {
best.fit = regsubsets(crim ~ ., data = Boston[folds != i, ], nvmax = p)
for (j in 1:p) {
pred = predict(best.fit, Boston[folds == i, ], id = j)
cv.errors[i, j] = mean((Boston$crim[folds == i] - pred)^2)
}
}
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 19, type = "b")
library(ISLR)
library(leaps)
attach(Hitters)
library(glmnet)
library(pls)
library(leaps)
library(MASS)
set.seed(1)
predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id = id)
mat[, names(coefi)] %*% coefi
}
k = 10
p = ncol(Boston) - 1
folds = sample(rep(1:k, length = nrow(Boston)))
cv.errors = matrix(NA, k, p)
for (i in 1:k) {
best.fit = regsubsets(crim ~ ., data = Boston[folds != i, ], nvmax = p)
for (j in 1:p) {
pred = predict(best.fit, Boston[folds == i, ], id = j)
cv.errors[i, j] = mean((Boston$crim[folds == i] - pred)^2)
}
}
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 19, type = "b")
which.min(rmse.cv)
rmse.cv[which.min(rmse.cv)]
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.lasso = cv.glmnet(x, y, type.measure = "mse")
plot(cv.lasso)
coef(cv.lasso)
sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.ridge = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.ridge)
coef(cv.ridge)
sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])
pcr.fit = pcr(crim ~ ., data = Boston, scale = TRUE, validation = "CV")
summary(pcr.fit)
cv.lasso$lambda
cv.lasso$lambda.min
cv.ridge$lambda == cv.ridge$lambda.1se
cvm[cv.ridge$lambda == cv.ridge$lambda.1se]
cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se]
cv.lasso$lambda
cv.lasso$lambda.min
cv.lasso = cv.glmnet(x, y)
cv.lasso$lambda
source('~/Stat_learning/Chater6_Exercise.R', echo=TRUE)
cv.lasso = cv.glmnet(x, y)
plot(cv.lasso)
coef(cv.ridge)
coef(cv.lasso)
x1 = 7.13
x2 = 5.26
x3 = 9.93
x4 = 6.62
x5 = 7.52
(x1+x2+x3+x4+x5)/5
(x1^2 + x2^2 + x3^2 + x4^2 + x5^2)/5
(x1^2 + x2^2 + x3^2 + x4^2 + x5^2)/5/5
(x1^2 + x2^2 + x3^2 + x4^2 + x5^2)/5
55.496 - (7.292)^2
7.292/2.3227
library(sqldf)
TXBirts_2004 = read.csv("Data/TXBirths2004.csv")
setwd("~/Mathematical_statistics")
TXBirts_2004 = read.csv("Data/TXBirths2004.csv")
head(TXBirts_2004)
count_string <- "select Smoker, count(*) from
TXBirts_2004 group by 1"
sqldf(count_string,stringsAsFactors = FALSE)
TXBirts_2004 = read.csv("Data/TXBirths2004.csv")
head(TXBirts_2004)
count_string <- "select Smoker, count(*) as count from
TXBirts_2004 group by 1"
sqldf(count_string,stringsAsFactors = FALSE)
Weight_Smoker = subset(TXBirts_2004,select = Weight, Smoker = "No", drop = TRUE)
Weight_Smoker = subset(TXBirts_2004,select = Weight, Smoker = "No", drop = TRUE)
Weight_NonSmoker = subset(TXBirts_2004,select = Weight, Smoker = "Yes", drop = TRUE)
Weight_Smoker
Weight_NonSmoker
hist(Weight_Smoker)
hist(Weight_NonSmoker)
hist(Weight_Smoker)
hist(Weight_NonSmoker)
qqnorm(Weight_Smoker)
qqline(Weight_Smoker)
qqnorm(Weight_NonSmoker)
qqline(Weight_NonSmoker)
length(TXBirts_2004)
nrow(TXBirts_2004)
nrow(Weight_Smoker)
length(Weight_Smoker)
Weight_Smoker = subset(TXBirts_2004,select = Weight, Smoker = "No", drop = T)
Weight_Smoker
Weight_Smoker = subset(TXBirts_2004,select = Weight, subset=Smoker = "No", drop = T)
Weight_Smoker = subset(TXBirts_2004,select = Weight, subset=Smoker == "No", drop = T)
Weight_NonSmoker = subset(TXBirts_2004,select = Weight,subset= Smoker == "Yes", drop = T)
hist(Weight_Smoker)
hist(Weight_NonSmoker)
qqnorm(Weight_Smoker)
qqline(Weight_Smoker)
qqnorm(Weight_NonSmoker)
qqline(Weight_NonSmoker)
t.test(Weight_NonSmoker,Weight_Smoker)$conf
print(lenghth(Weight_NonSmoker))
print(length(Weight_NonSmoker))
print(length(Weight_Smoker))
t.test(Weight_Smoker,Weight_NonSmoker)$conf
thetahat = mean(Weight_Smoker) - Weight_NonSmoker(Weight_NonSmoker)
thetahat = mean(Weight_Smoker) - mean(Weight_NonSmoker)
length(chlorine_bootstrap)
chlorine_bootstrap
chlorine_bootstrap = numeric(1)
length(chlorine_bootstrap)
chlorine_bootstrap[1]=0
chlorine_bootstrap[2]=2
length(chlorine_bootstrap)
