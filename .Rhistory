names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
sum(is.na(Hitters))
regfit.full = regsubsets(Salary~.,Hitters)
summary(regfit.full)
regfit.full = regsubsets(Salary ~., data = Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables" , ylab = "Adjusted Rsq", type = "l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col = "red",cex = 2,pch = 20)
plot(reg.summary$cp, xlab = " No of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10], col = "red",cex = 2,pch = 20)
plot(reg.summary$bic, xlab = " No of Variables", ylab = "BIC", type = "l")
which.min(reg.summary$bic)
points(6,reg.summary$bic[6], col = "red",cex = 2,pch = 20)
?plot.regsubsets
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
coef(regfit.full,6)
###
regfit.fwd = regsubsets(Salary ~.,data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary ~.,data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
# Choosing Among models using the validation set approach
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE)
train
train = sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE)
test = (!train)
regfit.best = regsubsets(Salary ~., data = Hitters[train,], nvmax = 19)
regfit.best = regsubsets(Salary ~., data = Hitters[train,], nvmax = 19)
test.mat = model.matrix(Salary~., data = Hitters[test, ])
test.mat
val.errors = rep(NA,19)
val.errors
coef(regfit.best, id =7)
coef(regfit.best, id =0)
coef(regfit.best, id =1)
coefi = coef(regfit.best, id =2)
coefi
v
names(coefi)
test.mat[, names(coefi)]
view(Hitters)
View(Hitters)
Hitters[test, ]
test.mat[, names(coefi)]%*%coefi
coefi
test.mat[, names(coefi)]
for (i in 1:19){
coefi = coef(regfit.best, id =i)
pred = test.mat[, names(coefi)]%*%coefi
val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
}
predict.regsubsets = function(object, newdata, id,...){
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id =id)
xvars = names(coefi)
mat[,xvars]%*%coefi
}
predict.regsubsets = function(object, newdata, id,...){
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id =id)
xvars = names(coefi)
mat[,xvars]%*%coefi
}
regfit.best = regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(regfit.best, 10)
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace = TRUE)
k =10
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace = TRUE)
folds
1:10
cv.errors = matrix(NA, k,19 )
cv.errors
paste(1:19)
cv.errors = matrix(NA, k,19, dimnames = 1:19 )
l i s t  (  N U L L  ,  p a s t e  ( 1 : 1 9 )
(Page 264).
list(1:10)
list(NULL, paste(1:19))
list(paste(1:19))
cv.errors = matrix(NA, k,19, dimnames = list(paste(1:19)))
cv.errors = matrix(NA, k,19, dimnames = list(NULL,paste(1:19)))
cv.errors
dimnames
list(NULL,paste(1:19))
cv.errors = matrix(NA, k,19, dimnames = list("test",paste(1:19)))
cv.errors = matrix(NA, k,19, dimnames = list(NULL,paste(1:19)))
for (j in 1:k){
best.fit = regsubsets(Salary ~., data = Hitters[folds!= j, ], nvmax = 19 )
for( i in 1:19){
pred = predict(best.fit, Hitters[folds == j,],id = i)
cv.errors[j,i] = mean((Hitters$Salary[folds ==j] - pred)^2)
}
}
mean.cv.errors = apply(cv.errors,2,mean)
mean.cv.errors = apply(cv.errors,2,mean)
mean.cv.errors
par(mforw = c(1,1))
par(mfrow = c(1,1))
plot(mean.cv.errors,type = 'b')
plot(mean.cv.errors,type = 'o')
par(mfrow = c(1,1))
plot(mean.cv.errors,type = 'b')
reg.best = regsubsets(Salary ~., data = Hitters, nvmax = 19)
coef(reg.best,11)
library(glmnet)
install.packages("glmnet")
library(glmnet)
library(glmnet)
x = model.matrix(Salary~., Hitters)[,-1]
y = Hitters$Salary
x
x = model.matrix(Salary~., Hitters)[,-2]
x
x = model.matrix(Salary~., Hitters)[,-1]
x
x = model.matrix(Salary~., Hitters)[,-2]
x
x = model.matrix(Salary~., Hitters)
x
x[1,]
x[2,]
x[,-1]
x[,-3]
x = model.matrix(Salary~., Hitters)[,-1]
y = Hitters$Salary
seq(10,-2, length = 100)
grid = 10^ seq(10,-2, length = 100)
grid
ridge.mod = glmnet(x,y alpha = 0, lamda = grid)
source('~/Personal/Stat_learning/Chater6_lab.R', echo=TRUE)
ridge.mod = glmnet(x,y ,alpha = 0, lamda = grid)
grid = 10^ seq(10,-2, length = 100)
ridge.mod = glmnet(x,y ,alpha = 0, lamda = grid)
ridge.mod = glmnet(x,y ,alpha = 0, lambda = grid)
library(ISLR)
library(leaps)
attach(Hitters)
library(glmnet)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
sum(is.na(Hitters))
regfit.full = regsubsets(Salary~.,Hitters)
summary(regfit.full)
regfit.full = regsubsets(Salary ~., data = Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables" , ylab = "Adjusted Rsq", type = "l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col = "red",cex = 2,pch = 20)
plot(reg.summary$cp, xlab = " No of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10], col = "red",cex = 2,pch = 20)
plot(reg.summary$bic, xlab = " No of Variables", ylab = "BIC", type = "l")
which.min(reg.summary$bic)
points(6,reg.summary$bic[6], col = "red",cex = 2,pch = 20)
?plot.regsubsets
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
coef(regfit.full,6)
###
regfit.fwd = regsubsets(Salary ~.,data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary ~.,data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
# Choosing Among models using the validation set approach
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE)
test = (!train)
regfit.best = regsubsets(Salary ~., data = Hitters[train,], nvmax = 19)
test.mat = model.matrix(Salary~., data = Hitters[test, ])
val.errors = rep(NA,19)
for (i in 1:19){
coefi = coef(regfit.best, id =i)
pred = test.mat[, names(coefi)]%*%coefi
val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
}
View(Hitters)
predict.regsubsets = function(object, newdata, id,...){
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id =id)
xvars = names(coefi)
mat[,xvars]%*%coefi
}
regfit.best = regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(regfit.best, 10)
k =10
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors = matrix(NA, k,19, dimnames = list(NULL,paste(1:19)))
for (j in 1:k){
best.fit = regsubsets(Salary ~., data = Hitters[folds!= j, ], nvmax = 19 )
for( i in 1:19){
pred = predict(best.fit, Hitters[folds == j,],id = i)
cv.errors[j,i] = mean((Hitters$Salary[folds ==j] - pred)^2)
}
}
mean.cv.errors = apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors,type = 'b')
reg.best = regsubsets(Salary ~., data = Hitters, nvmax = 19)
coef(reg.best,11)
x = model.matrix(Salary~., Hitters)[,-1]
y = Hitters$Salary
# Ridge regression
grid = 10^ seq(10,-2, length = 100)
ridge.mod = glmnet(x,y ,alpha = 0, lambda = grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
predict(ridge.mod, s= 50, type ="coefficients")[1:20]
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]
ridge.mod = glmnet( x[train, ], y[train] , alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred = predict(ridge.mod, s = 4 , newx = x[test, ])
mean((ridge.pred - y.test)^2)
mean((mean(y[train]) - y.test)^2)
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test, ])
mean((ridge.pred - y.test)^2)
ridge.pred = predict(ridge.mod,s =0,newx = x[test,], exact = T)
mean((ridge.pred - y.test)^2)
lm(y ~x , subset = train)
predict(ridge.mod, s= 0,exact = T, type ="coefficients")[1:20, ]
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
plot(cv.out)
plot(cv.out)
plot(cv.out)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha =0 )
bestlam = cv.out$lambda.min
print(bestlam)
ridge.pred = predict(ridge.mod, s= bestlam, newx = x[test, ])
mean((ridge.pred - y.test)^2)
out = glmnet(x,y alpha = 0)
out = glmnet(x,y ,alpha = 0)
predict(out, type= "coefficients", s= bestlam)[1:20,]
lasso.mod = glmnet(x[train,],y[train], alpha = 1)
lasso.mod = glmnet( x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.mod = glmnet(x[train,],y[train], alpha = 1)
source('~/Personal/Stat_learning/Chater6_lab.R', echo=TRUE)
plot(lasso.mod)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
source('~/Personal/Stat_learning/Chater6_lab.R', echo=TRUE)
mean((lasso.pred - y.test)^2)
out = glmnet(x,y, alpha = 1, lambda = grid)
source('~/Personal/Stat_learning/Chater6_lab.R', echo=TRUE)
lasso.coef
source('~/Personal/Stat_learning/Chater6_lab.R', echo=TRUE)
install.packages("pls")
library(pls)
library(pls)
set.seed(1)
pls.fit = plsr(Salary~., data = Hitters, subset = train, scale= TRUE, validation = "CV")
summary(pls.fit)
source('~/Personal/Stat_learning/Chater6_lab.R', echo=TRUE)
pcr.fit = pcr(Salary ~., data = Hitters, scale= TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type = "MSEP")
set.seed(1)
pcr.fit = pcr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = "CV")
pcr.fit = pcr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred = predict(pcr.fit, x[test,], ncomp = 7)
mean((pcr.pred - y.test)^2)
pcr.fit = pcr( y ~ x , scale= TRUE, ncomp = 7)
summary(pcr.fit)
set.seed(1)
pls.fit = plsr( Salary ~., data = Hitters, subset = train, scale = TRUE,
validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
mean((pls.pred - y.test)^2)
pls.pred = predict(pls.fit, x[test,] ,ncomp = 2)
mean((pls.pred - y.test)^2)
pls.fit = plsr(Salary ~., data = Hitters, scale = TRUE,ncomp = 2)
setwd("~/Personal/Mathematical_statistics")
##Chapter 5: Estimation
#----------------------------------------------------------
##Wind speed case study: fitting the Weibull distribution
#------------------------------------------------------------
# This function takes input shape parameter k and
# the data to compute
# (1/k)+ (1/n)*sum (log(xi)) +(1/alpha)sum xi^klog(xi)=0
# where alpha= sum xi^k.
weibull.shape <- function(k, data)
{
numer <- colSums(outer(data, k, "^") * log(data))
denom <- colSums(outer(data, k, "^"))
numer/denom - 1/k - mean(log(data))
}
weibull.scale <- function(k, data)
{
mean(data^k)^(1/k)
}
Turbine<- read.table("Data/Turbine.csv",header = TRUE,sep = ",")
wind <- Turbine$AveSpeed
uniroot(weibull.shape, data = wind, lower = 1,upper = 5)
weibull.scale(3.169, wind)
k
k =2
wind^k
weibull.scale(3.169, wind)
hist(wind, main = "Distribution of average wind speeds",
xlab = "meters/sec", prob = TRUE)
curve(dweibull(x, 3.169, 7.661), add = TRUE, col = "blue", lwd = 2)
plot.ecdf(wind,main = "ECDF of wind data")
curve(pweibull(x,3.169,7.661), add=TRUE, col="blue",lwd=2)
hist(wind, main = "Distribution of average wind speeds",
xlab = "meters/sec", prob = TRUE)
hist(wind, main = "Distribution of average wind speeds",
xlab = "meters/sec", prob = TRUE)
curve(dweibull(x, 3.169, 7.661), add = TRUE, col = "blue", lwd = 2)
x
hist(wind, main = "Distribution of average wind speeds",
xlab = "meters/sec", prob = TRUE)
curve(dweibull(x, 3.169, 7.661), add = TRUE, col = "blue", lwd = 2)
dev.new()
plot.ecdf(wind,main = "ECDF of wind data")
curve(pweibull(x,3.169,7.661), add=TRUE, col="blue",lwd=2)
plot.ecdf(wind,main = "ECDF of wind data")
curve(pweibull(x,3.169,7.661), add=TRUE, col="blue",lwd=2)
dev.new()
plot.ecdf(wind,main = "ECDF of wind data")
curve(pweibull(x,3.169,7.661), add=TRUE, col="blue",lwd=2)
q <- qweibull(seq(.1, .9, by = .1), 3.169, 7.661)
q
seq(.1, .9, by = .1)
range(wind)
q <- c(0, q, 14)
hist(wind, breaks=q, plot=F)$counts
# repeat above but store output
count <- hist(wind,breaks=q,plot=F)$counts
expected <- length(wind)*.1
# compute chi-square test statistic
sum((count-expected)^2/expected)
par(mfrow=c(1,2))
hist(my.mean, xlim=c(8,16),ylim=c(0,650),xlab="means",
main="2*Sample mean")
hist(my.max,  xlim=c(8,16),ylim=c(0,650),xlab="maximums",
main="25/24*maximum")
par(mfrow=c(1,1))
sum((count-expected)^2/expected)
# Simulation comparing two estimators for uniform
my.mean <- numeric(1000)
my.max <- numeric(1000)
#set.seed(100)
for (i in 1:1000)
{
x <- runif(25,0,12) #sample n=25 from Unif[0,1]
my.mean[i] <- 2*mean(x)
my.max[i] <- 26/25*max(x)
}
#mean and standard deviation of the method of moments estimate
mean(my.mean)
sd(my.mean)
#mean and sd of estimate from MLE
mean(my.max)
sd(my.max)
par(mfrow=c(1,2))
hist(my.mean, xlim=c(8,16),ylim=c(0,650),xlab="means",
main="2*Sample mean")
hist(my.max,  xlim=c(8,16),ylim=c(0,650),xlab="maximums",
main="25/24*maximum")
par(mfrow=c(1,1))
q <- c(0, q, 14)
q <- qweibull(seq(.1, .9, by = .1), 3.169, 7.661)
q
range(wind)
q <- c(0, q, 14)
q
hist(wind, breaks=q, plot=F)$counts
expected <- length(wind)*.1
sum((count-expected)^2/expected)
sum((count-expected)^2/expected)
numeric(1000)
# Example 6.13
# Simulation comparing two estimators for uniform
my.mean <- numeric(1000)
my.max <- numeric(1000)
#set.seed(100)
for (i in 1:1000)
{
x <- runif(25,0,12) #sample n=25 from Unif[0,1]
my.mean[i] <- 2*mean(x)
my.max[i] <- 26/25*max(x)
}
mean(my.mean)
sd(my.mean)
mean(my.max)
sd(my.max)
par(mfrow=c(1,2))
hist(my.mean, xlim=c(8,16),ylim=c(0,650),xlab="means",
main="2*Sample mean")
hist(my.max,  xlim=c(8,16),ylim=c(0,650),xlab="maximums",
main="25/24*maximum")
par(mfrow=c(1,1))
n <- length(wind)
theta1 <- mean(wind > 5)
theta1
eta1 <- quantile(wind, .1)
eta1
eta2 <- qweibull(.1, 3.169, 7.661)
eta2
eta1 <- quantile(wind, .1)
eta1
# nonparametric bootstrap to find standard errors
set.seed(23)
B <- 10^4
boot.theta1 <- numeric(B)
boot.shape <-  numeric(B)
boot.scale <-  numeric(B)
boot.theta2 <- numeric(B)
boot.eta1 <- numeric(B)
boot.eta2 <- numeric(B)
for (i in 1:B)
{
boot.wind <- wind[sample(1:n, replace=TRUE)]
boot.theta1[i] <- mean(boot.wind > 5)
boot.shape[i] <- uniroot(weibull.shape, data=boot.wind, lower=1,upper=5)$root
boot.scale[i] <- weibull.scale(boot.shape[i], boot.wind)
boot.theta2[i] <- pweibull(5, boot.shape[i], boot.scale[i], lower.t = FALSE)
boot.eta1[i] <- quantile(boot.wind, .1)
boot.eta2[i] <- qweibull(.1, boot.shape[i], boot.scale[i])
}
qqnorm(boot.theta1) # discrete, normal
qqnorm(boot.theta2) # continuous, normal
qqnorm(boot.eta1) # discrete, irregular, roughly normal
qqnorm(boot.eta2) # continuous, very normal
set.seed(23)
B <- 10^4
boot.theta1 <- numeric(B)
boot.shape <-  numeric(B)
boot.scale <-  numeric(B)
boot.theta2 <- numeric(B)
boot.eta1 <- numeric(B)
boot.eta2 <- numeric(B)
for (i in 1:B)
{
boot.wind <- wind[sample(1:n, replace=TRUE)]
boot.theta1[i] <- mean(boot.wind > 5)
boot.shape[i] <- uniroot(weibull.shape, data=boot.wind, lower=1,upper=5)$root
boot.scale[i] <- weibull.scale(boot.shape[i], boot.wind)
boot.theta2[i] <- pweibull(5, boot.shape[i], boot.scale[i], lower.t = FALSE)
boot.eta1[i] <- quantile(boot.wind, .1)
boot.eta2[i] <- qweibull(.1, boot.shape[i], boot.scale[i])
}
qqnorm(boot.theta1) # discrete, normal
qqnorm(boot.theta2) # continuous, normal
qqnorm(boot.eta1) # discrete, irregular, roughly normal
qqnorm(boot.eta2) # continuous, very normal
qqnorm(boot.theta1) # discrete, normal
qqnorm(boot.theta2) # continuous, normal
qqnorm(boot.eta1) # discrete, irregular, roughly normal
qqnorm(boot.eta2) # continuous, very normal
# Theta = P(wind > 5), nonparametric and parametric estimates
n <- length(wind)
theta1 <- mean(wind > 5)
theta2 <- pweibull(5, 3.169, 7.661, lower.tail = FALSE)
theta1 # 0.75
theta2 # 0.7720827
eta1 <- quantile(wind, .1)
eta2 <- qweibull(.1, 3.169, 7.661)
eta1 # 3.77
eta2 # 3.766038
qqnorm(boot.eta1) # discrete, irregular, roughly normal
qqnorm(boot.eta2) # continuous, very normal
qqnorm(boot.theta2) # continuous, normal
